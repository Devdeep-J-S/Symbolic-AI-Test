{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWmsYep7T20mbbwAfo21S5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devdeep-J-S/Symbolic-AI-Test/blob/main/Symbolic_AI_Tests_Task_2_Use_LSTM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name : Devdeep Shetranjiwala <br>\n",
        "Email ID : devdeep0702@gmail.com \n",
        "\n"
      ],
      "metadata": {
        "id": "Cuf0YIftMJG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Symbolic AI Tests: Task 2. Use LSTM model\n",
        "\n",
        "LSTM model in Keras to learn the Taylor expansion of a function"
      ],
      "metadata": {
        "id": "PhGqK6NfMUKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In this example, we first define the function we want to approximate (func) and its Taylor expansion up to a certain degree (taylor). We then generate training data by evaluating func on a grid of points, and computing the corresponding Taylor approximation.\n",
        "\n",
        "> We reshape the training data for the LSTM model, using a sequence length of seq_len = 10, and then define the LSTM model itself. We use a single LSTM layer with 128 units, followed by a dense layer with a single output.\n",
        "\n",
        "> We compile the model using mean squared error as the loss function and the Adam optimizer, and then fit the model on the training data.\n",
        "\n",
        "> After training, we can evaluate the model on a test set and compare its predictions to the true Taylor expansion of the function. Note that the quality of the approximation will depend on the degree of the Taylor expansion and the size of the training set."
      ],
      "metadata": {
        "id": "NFSgLdyyMt5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTX0OtaeMH5l",
        "outputId": "f1ea5b51-7158-49a7-d383-c9e6e4db4b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "6/6 [==============================] - 9s 6ms/step - loss: 0.4069\n",
            "Epoch 2/20\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0850\n",
            "Epoch 3/20\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0500\n",
            "Epoch 4/20\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0344\n",
            "Epoch 5/20\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0310\n",
            "Epoch 6/20\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0284\n",
            "Epoch 7/20\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0242\n",
            "Epoch 8/20\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0239\n",
            "Epoch 9/20\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0220\n",
            "Epoch 10/20\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0213\n",
            "Epoch 11/20\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0206\n",
            "Epoch 12/20\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0194\n",
            "Epoch 13/20\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0185\n",
            "Epoch 14/20\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0177\n",
            "Epoch 15/20\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0168\n",
            "Epoch 16/20\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0161\n",
            "Epoch 17/20\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0152\n",
            "Epoch 18/20\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0144\n",
            "Epoch 19/20\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0137\n",
            "Epoch 20/20\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0129\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb3d0093f10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "# Define the function to be approximated and its Taylor expansion\n",
        "def func(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "def taylor(x, n):\n",
        "    coeffs = np.zeros(n+1)\n",
        "    for i in range(n+1):\n",
        "        coeffs[i] = (-1)**i * x**(2*i+1) / np.math.factorial(2*i+1)\n",
        "    return np.sum(coeffs)\n",
        "\n",
        "# Generate training data\n",
        "x_train = np.linspace(-5, 5, 200)\n",
        "y_train = func(x_train)\n",
        "\n",
        "# Define the degree of the Taylor approximation\n",
        "degree = 5\n",
        "\n",
        "# Generate the Taylor approximation of the function\n",
        "taylor_train = np.zeros_like(y_train)\n",
        "for i in range(len(x_train)):\n",
        "    taylor_train[i] = taylor(x_train[i], degree)\n",
        "\n",
        "# Reshape the data for the LSTM model\n",
        "seq_len = 10\n",
        "num_features = 1\n",
        "num_sequences = len(x_train) - seq_len + 1\n",
        "\n",
        "x_train = np.zeros((num_sequences, seq_len, num_features))\n",
        "y_train = taylor_train\n",
        "\n",
        "for i in range(num_sequences):\n",
        "    x_train[i] = y_train[i:i+seq_len].reshape(seq_len, num_features)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(seq_len, num_features)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_train, y_train[:num_sequences], epochs=20, batch_size=32)"
      ]
    }
  ]
}