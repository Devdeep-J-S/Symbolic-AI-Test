{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIv7RfzDUr75efcpTEXAtL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devdeep-J-S/Symbolic-AI-Test/blob/main/Symbolic_AI_Tests_Task_3_Use_Transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name : Devdeep Shetranjiwala <br>\n",
        "Email ID : devdeep0702@gmail.com "
      ],
      "metadata": {
        "id": "69QXfR27O1w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Symbolic AI Tests : Task 3-Use Transformer model"
      ],
      "metadata": {
        "id": "2_s3eoHtO4TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> To train a Transformer model to learn the Taylor expansion of each function, we'll need to prepare a dataset of input-output pairs, where the input is a function and the output is its Taylor expansion. <br>\n",
        "Here's an example of how you could prepare such a dataset:"
      ],
      "metadata": {
        "id": "DktKp3Y7s2BC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input (function): sin(x)\n",
        "Output (Taylor expansion): x - x^3/3! + x^5/5! - x^7/7! + ...\n",
        "\n",
        "Input (function): cos(x)\n",
        "Output (Taylor expansion): 1 - x^2/2! + x^4/4! - x^6/6! + ...\n",
        "\n",
        "Input (function): exp(x)\n",
        "Output (Taylor expansion): 1 + x + x^2/2! + x^3/3! + ...\n",
        "\n",
        "Input (function): tan(x)\n",
        "Output (Taylor expansion): x + x^3/3 + 2x^5/15 + 17x^7/315 + ..."
      ],
      "metadata": {
        "id": "2J-vfdBwtJdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Once the dataset is done , using it to train a Transformer model to\n",
        "predict the Taylor expansion of a given function.<br>\n",
        "Here's an example using PyTorch:"
      ],
      "metadata": {
        "id": "Tfy7d5qes8s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Define the Transformer model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_layers, hidden_dim, num_heads, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.pos_encoding = PositionalEncoding(hidden_dim, dropout)\n",
        "        self.encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, hidden_dim, dropout)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layers, num_layers)\n",
        "        self.decoder = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Define the Positional Encoding module\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_dim, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, hidden_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, optimizer, criterion, dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for input, output in dataset:\n",
        "            optimizer.zero_grad()\n",
        "            input = torch.tensor(input, dtype=torch.long)\n",
        "            output = torch.tensor(output, dtype=torch.float)\n",
        "            prediction = model(input)\n",
        "            loss = criterion(prediction, output)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1} loss: {running_loss/len(dataset)}\")\n",
        "\n",
        "# Define the dataset\n",
        "dataset = [\n",
        "    ([1, 2, 3, 4, 5], [0, 1, 0, -1/6, 0]), # sin(x)\n",
        "    ([1, 2, 3, 4, 5], [1, 0, -1/2, 0, 1/24]), # cos(x)\n",
        "    ([1, 2, 3, 4, 5], [1, 1, 1/2, 1/6, 1/24]), # exp(x)\n",
        "    ([1, 2, 3, 4, 5], [1, 1/3, 2/15, 17/315, 62/2835]) # tan(x)\n",
        "]\n",
        "\n",
        "# Define the hyperparameters\n",
        "input_dim = 6 # number of functions to choose from + 1 (for padding)\n",
        "output_dim = 5 # length of the Taylor expansion\n",
        "num_layers = 4\n",
        "hidden_dim = 128\n",
        "num_heads = 8\n",
        "dropout = 0.1\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = TransformerModel(input_dim, output_dim, num_layers, hidden_dim, num_heads, dropout)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Train the model\n",
        "train(model, optimizer, criterion, dataset, epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPwdcX_hs7Hf",
        "outputId": "bf770fac-daef-41ca-96bb-0550a4e9ef1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 5, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 0.44324425607919693\n",
            "Epoch 2 loss: 0.2747147921472788\n",
            "Epoch 3 loss: 0.1893349066376686\n",
            "Epoch 4 loss: 0.14962004870176315\n",
            "Epoch 5 loss: 0.130391463637352\n",
            "Epoch 6 loss: 0.128837114199996\n",
            "Epoch 7 loss: 0.14354813192039728\n",
            "Epoch 8 loss: 0.1360676772892475\n",
            "Epoch 9 loss: 0.12003622949123383\n",
            "Epoch 10 loss: 0.12291123159229755\n",
            "Epoch 11 loss: 0.1197291761636734\n",
            "Epoch 12 loss: 0.1256052851676941\n",
            "Epoch 13 loss: 0.12614857126027346\n",
            "Epoch 14 loss: 0.11898323427885771\n",
            "Epoch 15 loss: 0.12189400754868984\n",
            "Epoch 16 loss: 0.1229244451969862\n",
            "Epoch 17 loss: 0.11761567555367947\n",
            "Epoch 18 loss: 0.12119482085108757\n",
            "Epoch 19 loss: 0.12213557958602905\n",
            "Epoch 20 loss: 0.12232185062021017\n",
            "Epoch 21 loss: 0.12014115694910288\n",
            "Epoch 22 loss: 0.12230431661009789\n",
            "Epoch 23 loss: 0.11855146009474993\n",
            "Epoch 24 loss: 0.11848580371588469\n",
            "Epoch 25 loss: 0.13054771441966295\n",
            "Epoch 26 loss: 0.11688564717769623\n",
            "Epoch 27 loss: 0.12030140683054924\n",
            "Epoch 28 loss: 0.11841667909175158\n",
            "Epoch 29 loss: 0.1170450747013092\n",
            "Epoch 30 loss: 0.11297056823968887\n",
            "Epoch 31 loss: 0.12114215362817049\n",
            "Epoch 32 loss: 0.12216584477573633\n",
            "Epoch 33 loss: 0.12051731534302235\n",
            "Epoch 34 loss: 0.11818937677890062\n",
            "Epoch 35 loss: 0.11907144635915756\n",
            "Epoch 36 loss: 0.11672065034508705\n",
            "Epoch 37 loss: 0.11759185325354338\n",
            "Epoch 38 loss: 0.111037059687078\n",
            "Epoch 39 loss: 0.11632787249982357\n",
            "Epoch 40 loss: 0.12182456534355879\n",
            "Epoch 41 loss: 0.11684448830783367\n",
            "Epoch 42 loss: 0.11977221723645926\n",
            "Epoch 43 loss: 0.11933332122862339\n",
            "Epoch 44 loss: 0.12278787512332201\n",
            "Epoch 45 loss: 0.11846382822841406\n",
            "Epoch 46 loss: 0.11653306661173701\n",
            "Epoch 47 loss: 0.12032783590257168\n",
            "Epoch 48 loss: 0.11609573476016521\n",
            "Epoch 49 loss: 0.1168378135189414\n",
            "Epoch 50 loss: 0.11634654272347689\n",
            "Epoch 51 loss: 0.11323279701173306\n",
            "Epoch 52 loss: 0.11855622008442879\n",
            "Epoch 53 loss: 0.11789526604115963\n",
            "Epoch 54 loss: 0.11410898808389902\n",
            "Epoch 55 loss: 0.11552436463534832\n",
            "Epoch 56 loss: 0.11457804497331381\n",
            "Epoch 57 loss: 0.11612088792026043\n",
            "Epoch 58 loss: 0.1144281281158328\n",
            "Epoch 59 loss: 0.11875011259689927\n",
            "Epoch 60 loss: 0.11613586451858282\n",
            "Epoch 61 loss: 0.11311477795243263\n",
            "Epoch 62 loss: 0.11595953535288572\n",
            "Epoch 63 loss: 0.113842798396945\n",
            "Epoch 64 loss: 0.11797579191625118\n",
            "Epoch 65 loss: 0.11529140546917915\n",
            "Epoch 66 loss: 0.11569269839674234\n",
            "Epoch 67 loss: 0.11576500348746777\n",
            "Epoch 68 loss: 0.11102611012756824\n",
            "Epoch 69 loss: 0.11771343648433685\n",
            "Epoch 70 loss: 0.1124407947063446\n",
            "Epoch 71 loss: 0.10986560210585594\n",
            "Epoch 72 loss: 0.11471550539135933\n",
            "Epoch 73 loss: 0.11423219088464975\n",
            "Epoch 74 loss: 0.11428232584148645\n",
            "Epoch 75 loss: 0.11570443585515022\n",
            "Epoch 76 loss: 0.11635111458599567\n",
            "Epoch 77 loss: 0.11448275484144688\n",
            "Epoch 78 loss: 0.11503275018185377\n",
            "Epoch 79 loss: 0.11359171103686094\n",
            "Epoch 80 loss: 0.1146872891113162\n",
            "Epoch 81 loss: 0.11422964837402105\n",
            "Epoch 82 loss: 0.1160111678764224\n",
            "Epoch 83 loss: 0.11297884583473206\n",
            "Epoch 84 loss: 0.11409921105951071\n",
            "Epoch 85 loss: 0.11463369242846966\n",
            "Epoch 86 loss: 0.1155912708491087\n",
            "Epoch 87 loss: 0.11690238863229752\n",
            "Epoch 88 loss: 0.11618676409125328\n",
            "Epoch 89 loss: 0.11580410040915012\n",
            "Epoch 90 loss: 0.1139894612133503\n",
            "Epoch 91 loss: 0.10914957989007235\n",
            "Epoch 92 loss: 0.11610193829983473\n",
            "Epoch 93 loss: 0.11644821893423796\n",
            "Epoch 94 loss: 0.11506267450749874\n",
            "Epoch 95 loss: 0.11405597440898418\n",
            "Epoch 96 loss: 0.11050948407500982\n",
            "Epoch 97 loss: 0.11592498794198036\n",
            "Epoch 98 loss: 0.11511959880590439\n",
            "Epoch 99 loss: 0.10996115766465664\n",
            "Epoch 100 loss: 0.11394576448947191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This code defines a dataset of four functions (sin(x), cos(x), exp(x), and tan(x)), with their corresponding Taylor expansions truncated to 5 terms.<br>\n",
        "The hyperparameters for the Transformer model are also defined, including the number of layers, hidden dimension, number of heads, and dropout rate. <br>\n",
        "The model is then initialized and trained using the train function, which takes in the model, optimizer, loss function, dataset, and number of epochs as arguments. <br>\n",
        "The train function loops over the dataset, computes the loss and gradients, and updates the model parameters using the optimizer. <br>\n",
        "After training is complete, the trained model can be used to predict the Taylor expansion of any function in the dataset."
      ],
      "metadata": {
        "id": "jOKn9ACftUlO"
      }
    }
  ]
}